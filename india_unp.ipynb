{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unemployment in India and Credit Card Spendings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data from two context:\n",
    "- Unemployment in India\n",
    "- Credit Card Spendings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_spendings = pd.read_csv(r'kaggle\\input\\Credit_Card_Transactions_date_changed.csv')\n",
    "df_unemployment = pd.read_csv(r'.\\kaggle\\input\\Unemployment in India.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unployment data\n",
    "profile_unemployment = ProfileReport(df_unemployment, title='Unemployment in India', explorative=True)\n",
    "\n",
    "# Spendings data\n",
    "profile_spendings = ProfileReport(df_spendings, title='Credit card transactions - India - Simple', explorative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the reports\n",
    "profile_unemployment.to_file(r'.\\profiles\\unemployment_data_profile.html')\n",
    "profile_spendings.to_file(r'.\\profiles\\spendings_data_profile.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrity constraints \n",
    "\n",
    "**Unemployment data**:  \n",
    "\n",
    "- We should have unique month-year for each row  \n",
    "- Percentage should be between 0 and 100  \n",
    "- There should be only Urban and Rural as values for Area\n",
    "\n",
    "**Spendings data**:\n",
    "\n",
    "- Amount should be positive and greater than 0 and not null\n",
    "- Date should be filled\n",
    "- City should be filled to allow integration with unemployment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify columns and range for missing fraction (0-10%)\n",
    "target_columns = ['City', 'Date', 'Amount']\n",
    "max_missing_fraction = 0.1  # Maximum 20% missing values\n",
    "\n",
    "for column in target_columns:\n",
    "    # Generate a random missing fraction between 0 and 20%\n",
    "    missing_fraction = np.random.uniform(0, max_missing_fraction)\n",
    "    \n",
    "    # Calculate number of missing values for this column\n",
    "    n_missing = int(missing_fraction * len(df_spendings))\n",
    "    \n",
    "    # Ensure that the fraction doesn't exceed the intended max missing count\n",
    "    n_missing = min(n_missing, int(max_missing_fraction * len(df_spendings)))\n",
    "    print(n_missing)\n",
    "\n",
    "    # Randomly select indices to set as NaN\n",
    "    missing_indices = np.random.choice(df_spendings.index, n_missing, replace=False)\n",
    "    \n",
    "    # Set selected indices in the column to NaN\n",
    "    df_spendings.loc[missing_indices, column] = np.nan\n",
    "\n",
    "# Display the number of missing values per column for confirmation\n",
    "print(\"Missing values introduced:\")\n",
    "print(df_spendings.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spendings data\n",
    "profile_spendings_missing_values = ProfileReport(df_spendings, title='Credit card transactions - India - Simple', explorative=True)\n",
    "profile_spendings_missing_values.to_file(r'.\\profiles\\spendings_data_profile_missing_values.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to handle missing values? \n",
    "1) add random value \n",
    "2) mean value\n",
    "3) interquartile range\n",
    "4) delete row  \n",
    "\n",
    "pros & cons for each one:\n",
    "1) pros: easy to apply with numpy \n",
    "   cons: without a range, I can add\n",
    "\n",
    "2) pros:    \n",
    "   cons:\n",
    "\n",
    "3) pros:\n",
    "   cons:\n",
    "\n",
    "4) pros:\n",
    "   cons:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings[df_spendings['City'] == \"Greater Mumbai, India\t\"][['Date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values in Unemployment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment['Region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment[df_unemployment['Region'] == 'Delhi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after watching this, we realized that we have 2 different estimated unemployment rate according to the area. However, since we're studying the credit card transactions, we believe that the best choice is to look only when the area is urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment = df_unemployment[df_unemployment['Area']=='Urban']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment['Region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.columns = df_unemployment.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['Area', 'Estimated Labour Participation Rate (%)', 'Estimated Employed', 'Frequency']\n",
    "df_unemployment = df_unemployment.drop(columns=columns_to_remove, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment = df_unemployment.applymap(lambda x: x.replace('\\n', '') if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment['Region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "required_dates = [\n",
    "    '31-05-2019', '30-06-2019', '31-07-2019', '31-08-2019',\n",
    "    '30-09-2019', '31-10-2019', '30-11-2019', '31-12-2019',\n",
    "    '31-01-2020', '29-02-2020', '31-03-2020', '30-04-2020',\n",
    "    '31-05-2020', '30-06-2020'\n",
    "]\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment[df_unemployment['Region']=='Jammu & Kashmir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define required dates\n",
    "required_dates = [\n",
    "    '31-05-2019', '30-06-2019', '31-07-2019', '31-08-2019',\n",
    "    '30-09-2019', '31-10-2019', '30-11-2019', '31-12-2019',\n",
    "    '31-01-2020', '29-02-2020', '31-03-2020', '30-04-2020',\n",
    "    '31-05-2020', '30-06-2020'\n",
    "]\n",
    "\n",
    "# Convert required dates to datetime format\n",
    "required_dates = pd.to_datetime(required_dates, format='%d-%m-%Y')\n",
    "\n",
    "# Convert the 'Date' column to datetime format in the dataset\n",
    "df_unemployment['Date'] = pd.to_datetime(df_unemployment['Date'], errors='coerce')\n",
    "\n",
    "# Get the unique regions\n",
    "unique_regions = df_unemployment['Region'].unique()\n",
    "\n",
    "# Initialize a list to collect rows with missing dates\n",
    "missing_dates_rows = []\n",
    "\n",
    "# Check for missing dates in each region and add rows as needed\n",
    "for region in unique_regions:\n",
    "    region_data = df_unemployment[df_unemployment['Region'] == region]\n",
    "    region_dates = region_data['Date'].unique()\n",
    "    \n",
    "    # Identify missing dates\n",
    "    missing_dates = set(required_dates) - set(region_dates)\n",
    "    \n",
    "    # Create new rows for missing dates\n",
    "    for missing_date in missing_dates:\n",
    "        missing_dates_rows.append({\n",
    "            'Region': region,\n",
    "            'Date': missing_date,\n",
    "            'Frequency': None,\n",
    "            'Estimated Unemployment Rate (%)': None,\n",
    "            'Estimated Employed': None,\n",
    "            'Estimated Labour Participation Rate (%)': None,\n",
    "            'Area': None\n",
    "        })\n",
    "\n",
    "# Convert missing dates list to a DataFrame and append to the original data\n",
    "missing_dates_df = pd.DataFrame(missing_dates_rows)\n",
    "df_unemployment = pd.concat([df_unemployment, missing_dates_df], ignore_index=True)\n",
    "\n",
    "# Sort the data by Region and Date for better readability\n",
    "df_unemployment = df_unemployment.sort_values(by=['Region', 'Date']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment['Region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unemployment[df_unemployment['Region']=='Jammu & Kashmir']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data into df_unemployment (assuming this DataFrame is already defined)\n",
    "# df_unemployment = pd.read_csv(\"your_data.csv\")  # Uncomment and specify your file if needed\n",
    "\n",
    "# Convert Date to datetime format\n",
    "df_unemployment['Date'] = pd.to_datetime(df_unemployment['Date'], errors='coerce')\n",
    "\n",
    "# Sort by Region and Date to maintain order\n",
    "df_unemployment = df_unemployment.sort_values(by=['Region', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create a copy of the data to store imputed values\n",
    "df_imputed = df_unemployment.copy()\n",
    "\n",
    "# Store metrics results\n",
    "mae_list = []\n",
    "r2_list = []\n",
    "\n",
    "# Apply imputation by region\n",
    "regions = df_unemployment['Region'].unique()\n",
    "\n",
    "for region in regions:\n",
    "    # Select data for the current region\n",
    "    region_data = df_unemployment[df_unemployment['Region'] == region]\n",
    "    \n",
    "    # Extract indices of missing and non-missing values\n",
    "    missing_indices = region_data['Estimated Unemployment Rate (%)'].isna()\n",
    "    non_missing_indices = ~missing_indices\n",
    "\n",
    "    # Continue only if there are at least two known values to fit the model\n",
    "    if non_missing_indices.sum() > 1:  # We need at least two points to fit a linear model\n",
    "        # Prepare data for regression\n",
    "        X = region_data.loc[non_missing_indices, 'Date'].map(pd.Timestamp.toordinal).values.reshape(-1, 1)\n",
    "        y = region_data.loc[non_missing_indices, 'Estimated Unemployment Rate (%)'].values\n",
    "        \n",
    "        # Perform a 70-30 train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Fit the model on training data\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics on test data\n",
    "        mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        r2 = r2_score(y_test, y_pred_test)\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        mae_list.append(mae)\n",
    "        r2_list.append(r2)\n",
    "        \n",
    "        # Predict missing values if any\n",
    "        if missing_indices.sum() > 0:\n",
    "            X_missing = region_data.loc[missing_indices, 'Date'].map(pd.Timestamp.toordinal).values.reshape(-1, 1)\n",
    "            y_pred_missing = model.predict(X_missing)\n",
    "            \n",
    "            # Assign predictions only for rows where `missing_indices` is True in `df_imputed`\n",
    "            df_imputed.loc[region_data.index[missing_indices], 'Estimated Unemployment Rate (%)'] = y_pred_missing\n",
    "\n",
    "# Calculate and display average metrics\n",
    "print(f\"Mean Absolute Error (MAE): {np.mean(mae_list):.4f}\")\n",
    "print(f\"R-squared (R²): {np.mean(r2_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values in spendings data from india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma = df_spendings.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['Gender', 'Exp Type', 'Card Type', 'df_index']\n",
    "df_spendings_ma = df_spendings_ma.drop(columns=columns_to_remove, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma['City'] = df_spendings_ma['City'].str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma['City'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the top cities\n",
    "top_cities = df_spendings_ma['City'].value_counts().index[:4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random choices only for NaN values in the 'City' column\n",
    "random_cities = np.random.choice(top_cities, size=df_spendings_ma['City'].isna().sum())\n",
    "\n",
    "# Assign these random cities to the NaN values in the 'City' column only\n",
    "df_spendings_ma.loc[df_spendings['City'].isna(), 'City'] = random_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the overall mode of the Date column as a fallback\n",
    "overall_mode_date = df_spendings_ma['Date'].mode()[0] if not df_spendings_ma['Date'].mode().empty else None\n",
    "\n",
    "# Fill missing dates with the mode for each city or with the overall mode if a city has no dates\n",
    "df_spendings_ma['Date'] = df_spendings_ma.groupby('City')['Date'].transform(\n",
    "    lambda x: x.fillna(x.mode()[0] if not x.mode().empty else overall_mode_date)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_spendings_ma.dropna(subset=['Amount'])\n",
    "\n",
    "# Step 2: Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = df_cleaned['Amount'].quantile(0.25)\n",
    "Q3 = df_cleaned['Amount'].quantile(0.75)\n",
    "\n",
    "# Step 3: Calculate IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Step 4: Fill missing values in 'Amount' with the IQR\n",
    "df_spendings_ma['Amount'] = df_spendings_ma['Amount'].fillna(IQR)\n",
    "\n",
    "# Display the result to confirm the missing values have been filled\n",
    "print(df_spendings_ma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spendings_ma.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add region in spending dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_india_cities = pd.read_csv(\"kaggle\\input\\cities_state_india.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_india_cities = df_india_cities.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_india_cities = df_india_cities.loc[df_india_cities.groupby('City')['Population (2011)'].idxmax()]\n",
    "\n",
    "# Reset index for a clean output\n",
    "df_india_cities = df_india_cities.reset_index(drop=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_india_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of common alternate city names for normalization\n",
    "city_name_mapping = {\n",
    "    \"Bengaluru\": \"Bangalore\"\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Function to normalize city names\n",
    "def normalize_city_name(city_name):\n",
    "    # Check if the city name has an alternate in the dictionary\n",
    "    return city_name_mapping.get(city_name, city_name)\n",
    "\n",
    "# Update the get_region function to include normalization\n",
    "def get_region_with_normalization(city_name):\n",
    "    # Normalize the city name\n",
    "    normalized_city = normalize_city_name(city_name)\n",
    "    \n",
    "    # Match based on \"contains\" logic\n",
    "    for index, row in df_india_cities.iterrows():\n",
    "        if row['City'].lower() in normalized_city.lower() or normalized_city.lower() in row['City'].lower():\n",
    "            return row['State or union territory']\n",
    "    return None\n",
    "\n",
    "# Apply the updated function to add a new 'Region' column to df_spending_region\n",
    "df_spendings_ma['Region'] = df_spendings_ma['City'].apply(get_region_with_normalization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste = df_spendings_ma[df_spendings_ma.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
